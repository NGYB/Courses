{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 1.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check your versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy 1.13.1\n",
      "pandas 0.20.3\n",
      "scipy 0.19.1\n",
      "sklearn 0.19.0\n",
      "lightgbm 2.0.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sklearn\n",
    "import scipy.sparse \n",
    "import lightgbm \n",
    "\n",
    "for p in [np, pd, scipy, sklearn, lightgbm]:\n",
    "    print (p.__name__, p.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important!** There is a huge chance that the assignment will be impossible to pass if the versions of `lighgbm` and `scikit-learn` are wrong. The versions being tested:\n",
    "\n",
    "    numpy 1.13.1\n",
    "    pandas 0.20.3\n",
    "    scipy 0.19.1\n",
    "    sklearn 0.19.0\n",
    "    ligthgbm 2.0.6\n",
    "    \n",
    "\n",
    "To install an older version of `lighgbm` you may use the following command:\n",
    "```\n",
    "pip uninstall lightgbm\n",
    "pip install lightgbm==2.0.6\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this programming assignment you are asked to implement two ensembling schemes: simple linear mix and stacking.\n",
    "\n",
    "We will spend several cells to load data and create feature matrix, you can scroll down this part or try to understand what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "pd.set_option('display.max_rows', 600)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "def downcast_dtypes(df):\n",
    "    '''\n",
    "        Changes column types in the dataframe: \n",
    "                \n",
    "                `float64` type to `float32`\n",
    "                `int64`   type to `int32`\n",
    "    '''\n",
    "    \n",
    "    # Select columns to downcast\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n",
    "    \n",
    "    # Downcast\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols]   = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data from the hard drive first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv('../readonly/final_project_data/sales_train.csv.gz')\n",
    "shops = pd.read_csv('../readonly/final_project_data/shops.csv')\n",
    "items = pd.read_csv('../readonly/final_project_data/items.csv')\n",
    "item_cats = pd.read_csv('../readonly/final_project_data/item_categories.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_price</th>\n",
       "      <th>item_cnt_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02.01.2013</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>22154</td>\n",
       "      <td>999.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>03.01.2013</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>2552</td>\n",
       "      <td>899.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>05.01.2013</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>2552</td>\n",
       "      <td>899.00</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>06.01.2013</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>2554</td>\n",
       "      <td>1709.05</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.01.2013</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>2555</td>\n",
       "      <td>1099.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  date_block_num  shop_id  item_id  item_price  item_cnt_day\n",
       "0  02.01.2013               0       59    22154      999.00           1.0\n",
       "1  03.01.2013               0       25     2552      899.00           1.0\n",
       "2  05.01.2013               0       25     2552      899.00          -1.0\n",
       "3  06.01.2013               0       25     2554     1709.05           1.0\n",
       "4  15.01.2013               0       25     2555     1099.00           1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use only 3 shops for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = sales[sales['shop_id'].isin([26, 27, 28])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get a feature matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to prepare the features. This part is all implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/pandas/core/groupby.py:4036: FutureWarning: using a dict with renaming is deprecated and will be removed in a future version\n",
      "  return super(DataFrameGroupBy, self).aggregate(arg, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Create \"grid\" with columns\n",
    "index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "\n",
    "# For every month we create a grid from all shops/items combinations from that month\n",
    "grid = [] \n",
    "for block_num in sales['date_block_num'].unique():\n",
    "    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n",
    "    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n",
    "    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n",
    "# product: cartesian product, equivalent to a nested for-loop\n",
    "# *[a,b,c]: splits the sequence into separate arguments for the function call.\n",
    "\n",
    "# Turn the grid into a dataframe\n",
    "grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n",
    "# vstack: Stack arrays in sequence vertically (row wise).\n",
    "#   shop_id\titem_id\tdate_block_num\n",
    "# 0\t59\t22154\t0\n",
    "# 1\t59\t2552\t0\n",
    "# 2\t59\t2554\t0\n",
    "# 3\t59\t2555\t0\n",
    "\n",
    "# Groupby data to get shop-item-month aggregates\n",
    "gb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target':'sum'}})\n",
    "# shop_id\titem_id\tdate_block_num\titem_cnt_day\n",
    "#                                   target\n",
    "# 0\t0\t30\t1\t31.0\n",
    "# 1\t0\t31\t1\t11.0\n",
    "# 2\t0\t32\t0\t6.0\n",
    "# 3\t0\t32\t1\t10.0\n",
    "\n",
    "# Fix column names\n",
    "# gb.columns: MultiIndex(levels=[['item_cnt_day', 'date_block_num', 'item_id', 'shop_id'], ['target', '']],\n",
    "#            labels=[[3, 2, 1, 0], [1, 1, 1, 0]])\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] \n",
    "# Index(['shop_id', 'item_id', 'date_block_num', 'target'], dtype='object')\n",
    "\n",
    "# # Join it to the grid\n",
    "all_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n",
    "#   shop_id\titem_id\tdate_block_num\ttarget\n",
    "# 0\t28\t7738\t0\t4.0\n",
    "# 1\t28\t7737\t0\t10.0\n",
    "# 2\t28\t7770\t0\t6.0\n",
    "# 3\t28\t7664\t0\t1.0\n",
    "\n",
    "# Same as above but with shop-month aggregates\n",
    "gb = sales.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_shop':'sum'}})\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "# Same as above but with item-month aggregates\n",
    "gb = sales.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_item':'sum'}})\n",
    "gb.columns = [col[0] if col[-1] == '' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "# Downcast dtypes from 64 to 32 bit to save memory\n",
    "all_data = downcast_dtypes(all_data)\n",
    "del grid, gb \n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>target</th>\n",
       "      <th>target_shop</th>\n",
       "      <th>target_item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>7738</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7057.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>7737</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7057.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>7770</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7057.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>7664</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7057.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>7814</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7057.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shop_id  item_id  date_block_num  target  target_shop  target_item\n",
       "0       28     7738               0     4.0       7057.0         11.0\n",
       "1       28     7737               0    10.0       7057.0         16.0\n",
       "2       28     7770               0     6.0       7057.0         10.0\n",
       "3       28     7664               0     1.0       7057.0          1.0\n",
       "4       28     7814               0     2.0       7057.0          6.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating a grid, we can calculate some features. We will use lags from [1, 2, 3, 4, 5, 12] months ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# List of columns that we will use to create lags\n",
    "cols_to_rename = list(all_data.columns.difference(index_cols)) \n",
    "\n",
    "shift_range = [1, 2, 3, 4, 5, 12]\n",
    "\n",
    "for month_shift in tqdm_notebook(shift_range):\n",
    "    train_shift = all_data[index_cols + cols_to_rename].copy()\n",
    "    \n",
    "    train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n",
    "    \n",
    "    foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n",
    "    train_shift = train_shift.rename(columns=foo)\n",
    "\n",
    "    all_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)\n",
    "\n",
    "del train_shift\n",
    "\n",
    "# Don't use old data from year 2013\n",
    "all_data = all_data[all_data['date_block_num'] >= 12] \n",
    "\n",
    "# List of all lagged features\n",
    "fit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]] \n",
    "# We will drop these at fitting stage\n",
    "to_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols))) + ['date_block_num'] \n",
    "\n",
    "# Category for each item\n",
    "item_category_mapping = items[['item_id','item_category_id']].drop_duplicates()\n",
    "\n",
    "all_data = pd.merge(all_data, item_category_mapping, how='left', on='item_id')\n",
    "all_data = downcast_dtypes(all_data)\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this end, we've created a feature matrix. It is stored in `all_data` variable. Take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>target</th>\n",
       "      <th>target_shop</th>\n",
       "      <th>target_item</th>\n",
       "      <th>target_lag_1</th>\n",
       "      <th>target_item_lag_1</th>\n",
       "      <th>target_shop_lag_1</th>\n",
       "      <th>target_lag_2</th>\n",
       "      <th>target_item_lag_2</th>\n",
       "      <th>target_shop_lag_2</th>\n",
       "      <th>target_lag_3</th>\n",
       "      <th>target_item_lag_3</th>\n",
       "      <th>target_shop_lag_3</th>\n",
       "      <th>target_lag_4</th>\n",
       "      <th>target_item_lag_4</th>\n",
       "      <th>target_shop_lag_4</th>\n",
       "      <th>target_lag_5</th>\n",
       "      <th>target_item_lag_5</th>\n",
       "      <th>target_shop_lag_5</th>\n",
       "      <th>target_lag_12</th>\n",
       "      <th>target_item_lag_12</th>\n",
       "      <th>target_shop_lag_12</th>\n",
       "      <th>item_category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>10994</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6949.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6454.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>10992</td>\n",
       "      <td>12</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6949.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7521.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>10991</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6949.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5609.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6753.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7521.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>10988</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6949.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6454.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5609.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6753.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>11002</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6949.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shop_id  item_id  date_block_num  target  target_shop  target_item  \\\n",
       "0       28    10994              12     1.0       6949.0          1.0   \n",
       "1       28    10992              12     3.0       6949.0          4.0   \n",
       "2       28    10991              12     1.0       6949.0          5.0   \n",
       "3       28    10988              12     1.0       6949.0          2.0   \n",
       "4       28    11002              12     1.0       6949.0          1.0   \n",
       "\n",
       "   target_lag_1  target_item_lag_1  target_shop_lag_1  target_lag_2  \\\n",
       "0           0.0                1.0             8499.0           0.0   \n",
       "1           3.0                7.0             8499.0           0.0   \n",
       "2           1.0                3.0             8499.0           0.0   \n",
       "3           2.0                5.0             8499.0           4.0   \n",
       "4           0.0                1.0             8499.0           0.0   \n",
       "\n",
       "   target_item_lag_2  target_shop_lag_2  target_lag_3  target_item_lag_3  \\\n",
       "0                1.0             6454.0           0.0                0.0   \n",
       "1                0.0                0.0           0.0                0.0   \n",
       "2                0.0                0.0           0.0                1.0   \n",
       "3                5.0             6454.0           5.0                6.0   \n",
       "4                0.0                0.0           0.0                0.0   \n",
       "\n",
       "   target_shop_lag_3  target_lag_4  target_item_lag_4  target_shop_lag_4  \\\n",
       "0                0.0           0.0                0.0                0.0   \n",
       "1                0.0           0.0                0.0                0.0   \n",
       "2             5609.0           0.0                2.0             6753.0   \n",
       "3             5609.0           0.0                2.0             6753.0   \n",
       "4                0.0           0.0                0.0                0.0   \n",
       "\n",
       "   target_lag_5  target_item_lag_5  target_shop_lag_5  target_lag_12  \\\n",
       "0           0.0                0.0                0.0            0.0   \n",
       "1           0.0                1.0             7521.0            0.0   \n",
       "2           2.0                4.0             7521.0            0.0   \n",
       "3           0.0                0.0                0.0            0.0   \n",
       "4           0.0                0.0                0.0            0.0   \n",
       "\n",
       "   target_item_lag_12  target_shop_lag_12  item_category_id  \n",
       "0                 0.0                 0.0                37  \n",
       "1                 0.0                 0.0                37  \n",
       "2                 0.0                 0.0                40  \n",
       "3                 0.0                 0.0                40  \n",
       "4                 0.0                 0.0                40  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28,\n",
       "       29, 30, 31, 32, 33])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.date_block_num.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 157950 entries, 0 to 157949\n",
      "Data columns (total 25 columns):\n",
      "shop_id               157950 non-null int32\n",
      "item_id               157950 non-null int32\n",
      "date_block_num        157950 non-null int32\n",
      "target                157950 non-null float32\n",
      "target_shop           157950 non-null float32\n",
      "target_item           157950 non-null float32\n",
      "target_lag_1          157950 non-null float32\n",
      "target_item_lag_1     157950 non-null float32\n",
      "target_shop_lag_1     157950 non-null float32\n",
      "target_lag_2          157950 non-null float32\n",
      "target_item_lag_2     157950 non-null float32\n",
      "target_shop_lag_2     157950 non-null float32\n",
      "target_lag_3          157950 non-null float32\n",
      "target_item_lag_3     157950 non-null float32\n",
      "target_shop_lag_3     157950 non-null float32\n",
      "target_lag_4          157950 non-null float32\n",
      "target_item_lag_4     157950 non-null float32\n",
      "target_shop_lag_4     157950 non-null float32\n",
      "target_lag_5          157950 non-null float32\n",
      "target_item_lag_5     157950 non-null float32\n",
      "target_shop_lag_5     157950 non-null float32\n",
      "target_lag_12         157950 non-null float32\n",
      "target_item_lag_12    157950 non-null float32\n",
      "target_shop_lag_12    157950 non-null float32\n",
      "item_category_id      157950 non-null int32\n",
      "dtypes: float32(21), int32(4)\n",
      "memory usage: 16.3 MB\n"
     ]
    }
   ],
   "source": [
    "all_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all_data:\n",
    " * **target** means the number of items sold for this shop_id-item_id-date_block_num combination\n",
    " * **target_shop** means the number of items sold for this shop_id-date_block_num combination. Note target_shop > target\n",
    " * **target_item** means the number of items sold for this item_id-date_block_num combination. Note target_item > target\n",
    " * **target_lag_1** means the number of items sold for this shop_id-item_id-(date_block_num-1) combination. e.g. if data_block_num = 12, then target_lag_1 is for date_block_num = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a sake of the programming assignment, let's artificially split the data into train and test. We will treat last month data as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test `date_block_num` is 33\n"
     ]
    }
   ],
   "source": [
    "# Save `date_block_num`, as we can't use them as features, but will need them to split the dataset into parts \n",
    "dates = all_data['date_block_num']\n",
    "\n",
    "last_block = dates.max()\n",
    "print('Test `date_block_num` is %d' % last_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_train = dates[dates <  last_block]\n",
    "dates_test  = dates[dates == last_block]\n",
    "\n",
    "X_train = all_data.loc[dates <  last_block].drop(to_drop_cols, axis=1)\n",
    "X_test =  all_data.loc[dates == last_block].drop(to_drop_cols, axis=1)\n",
    "\n",
    "y_train = all_data.loc[dates <  last_block, 'target'].values\n",
    "y_test =  all_data.loc[dates == last_block, 'target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>target_lag_1</th>\n",
       "      <th>target_item_lag_1</th>\n",
       "      <th>target_shop_lag_1</th>\n",
       "      <th>target_lag_2</th>\n",
       "      <th>target_item_lag_2</th>\n",
       "      <th>target_shop_lag_2</th>\n",
       "      <th>target_lag_3</th>\n",
       "      <th>target_item_lag_3</th>\n",
       "      <th>target_shop_lag_3</th>\n",
       "      <th>target_lag_4</th>\n",
       "      <th>target_item_lag_4</th>\n",
       "      <th>target_shop_lag_4</th>\n",
       "      <th>target_lag_5</th>\n",
       "      <th>target_item_lag_5</th>\n",
       "      <th>target_shop_lag_5</th>\n",
       "      <th>target_lag_12</th>\n",
       "      <th>target_item_lag_12</th>\n",
       "      <th>target_shop_lag_12</th>\n",
       "      <th>item_category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>10994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6454.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>10992</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7521.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>10991</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5609.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6753.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7521.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>10988</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6454.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5609.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6753.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>11002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shop_id  item_id  target_lag_1  target_item_lag_1  target_shop_lag_1  \\\n",
       "0       28    10994           0.0                1.0             8499.0   \n",
       "1       28    10992           3.0                7.0             8499.0   \n",
       "2       28    10991           1.0                3.0             8499.0   \n",
       "3       28    10988           2.0                5.0             8499.0   \n",
       "4       28    11002           0.0                1.0             8499.0   \n",
       "\n",
       "   target_lag_2  target_item_lag_2  target_shop_lag_2  target_lag_3  \\\n",
       "0           0.0                1.0             6454.0           0.0   \n",
       "1           0.0                0.0                0.0           0.0   \n",
       "2           0.0                0.0                0.0           0.0   \n",
       "3           4.0                5.0             6454.0           5.0   \n",
       "4           0.0                0.0                0.0           0.0   \n",
       "\n",
       "   target_item_lag_3  target_shop_lag_3  target_lag_4  target_item_lag_4  \\\n",
       "0                0.0                0.0           0.0                0.0   \n",
       "1                0.0                0.0           0.0                0.0   \n",
       "2                1.0             5609.0           0.0                2.0   \n",
       "3                6.0             5609.0           0.0                2.0   \n",
       "4                0.0                0.0           0.0                0.0   \n",
       "\n",
       "   target_shop_lag_4  target_lag_5  target_item_lag_5  target_shop_lag_5  \\\n",
       "0                0.0           0.0                0.0                0.0   \n",
       "1                0.0           0.0                1.0             7521.0   \n",
       "2             6753.0           2.0                4.0             7521.0   \n",
       "3             6753.0           0.0                0.0                0.0   \n",
       "4                0.0           0.0                0.0                0.0   \n",
       "\n",
       "   target_lag_12  target_item_lag_12  target_shop_lag_12  item_category_id  \n",
       "0            0.0                 0.0                 0.0                37  \n",
       "1            0.0                 0.0                 0.0                37  \n",
       "2            0.0                 0.0                 0.0                40  \n",
       "3            0.0                 0.0                 0.0                40  \n",
       "4            0.0                 0.0                 0.0                40  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 154596 entries, 0 to 154595\n",
      "Data columns (total 21 columns):\n",
      "shop_id               154596 non-null int32\n",
      "item_id               154596 non-null int32\n",
      "target_lag_1          154596 non-null float32\n",
      "target_item_lag_1     154596 non-null float32\n",
      "target_shop_lag_1     154596 non-null float32\n",
      "target_lag_2          154596 non-null float32\n",
      "target_item_lag_2     154596 non-null float32\n",
      "target_shop_lag_2     154596 non-null float32\n",
      "target_lag_3          154596 non-null float32\n",
      "target_item_lag_3     154596 non-null float32\n",
      "target_shop_lag_3     154596 non-null float32\n",
      "target_lag_4          154596 non-null float32\n",
      "target_item_lag_4     154596 non-null float32\n",
      "target_shop_lag_4     154596 non-null float32\n",
      "target_lag_5          154596 non-null float32\n",
      "target_item_lag_5     154596 non-null float32\n",
      "target_shop_lag_5     154596 non-null float32\n",
      "target_lag_12         154596 non-null float32\n",
      "target_item_lag_12    154596 non-null float32\n",
      "target_shop_lag_12    154596 non-null float32\n",
      "item_category_id      154596 non-null int32\n",
      "dtypes: float32(18), int32(3)\n",
      "memory usage: 13.6 MB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  3.,  1., ...,  0.,  0.,  0.], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First level models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to implement a basic stacking scheme. We have a time component here, so we will use ***scheme f)*** from the reading material. Recall, that we always use first level models to build two datasets: test meta-features and 2-nd level train-metafetures. Let's see how we get test meta-features first. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test meta-features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firts, we will run *linear regression* on numeric columns and get predictions for the last month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R-squared for linreg is 0.743180\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train.values, y_train)\n",
    "pred_lr = lr.predict(X_test.values)\n",
    "\n",
    "print('Test R-squared for linreg is %f' % r2_score(y_test, pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the we run *LightGBM*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  3.,  1., ...,  0.,  0.,  0.], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>target_lag_1</th>\n",
       "      <th>target_item_lag_1</th>\n",
       "      <th>target_shop_lag_1</th>\n",
       "      <th>target_lag_2</th>\n",
       "      <th>target_item_lag_2</th>\n",
       "      <th>target_shop_lag_2</th>\n",
       "      <th>target_lag_3</th>\n",
       "      <th>target_item_lag_3</th>\n",
       "      <th>target_shop_lag_3</th>\n",
       "      <th>target_lag_4</th>\n",
       "      <th>target_item_lag_4</th>\n",
       "      <th>target_shop_lag_4</th>\n",
       "      <th>target_lag_5</th>\n",
       "      <th>target_item_lag_5</th>\n",
       "      <th>target_shop_lag_5</th>\n",
       "      <th>target_lag_12</th>\n",
       "      <th>target_item_lag_12</th>\n",
       "      <th>target_shop_lag_12</th>\n",
       "      <th>item_category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>10994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6454.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>10992</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7521.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>10991</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5609.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6753.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7521.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>10988</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6454.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5609.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6753.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>11002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shop_id  item_id  target_lag_1  target_item_lag_1  target_shop_lag_1  \\\n",
       "0       28    10994           0.0                1.0             8499.0   \n",
       "1       28    10992           3.0                7.0             8499.0   \n",
       "2       28    10991           1.0                3.0             8499.0   \n",
       "3       28    10988           2.0                5.0             8499.0   \n",
       "4       28    11002           0.0                1.0             8499.0   \n",
       "\n",
       "   target_lag_2  target_item_lag_2  target_shop_lag_2  target_lag_3  \\\n",
       "0           0.0                1.0             6454.0           0.0   \n",
       "1           0.0                0.0                0.0           0.0   \n",
       "2           0.0                0.0                0.0           0.0   \n",
       "3           4.0                5.0             6454.0           5.0   \n",
       "4           0.0                0.0                0.0           0.0   \n",
       "\n",
       "   target_item_lag_3  target_shop_lag_3  target_lag_4  target_item_lag_4  \\\n",
       "0                0.0                0.0           0.0                0.0   \n",
       "1                0.0                0.0           0.0                0.0   \n",
       "2                1.0             5609.0           0.0                2.0   \n",
       "3                6.0             5609.0           0.0                2.0   \n",
       "4                0.0                0.0           0.0                0.0   \n",
       "\n",
       "   target_shop_lag_4  target_lag_5  target_item_lag_5  target_shop_lag_5  \\\n",
       "0                0.0           0.0                0.0                0.0   \n",
       "1                0.0           0.0                1.0             7521.0   \n",
       "2             6753.0           2.0                4.0             7521.0   \n",
       "3             6753.0           0.0                0.0                0.0   \n",
       "4                0.0           0.0                0.0                0.0   \n",
       "\n",
       "   target_lag_12  target_item_lag_12  target_shop_lag_12  item_category_id  \n",
       "0            0.0                 0.0                 0.0                37  \n",
       "1            0.0                 0.0                 0.0                37  \n",
       "2            0.0                 0.0                 0.0                40  \n",
       "3            0.0                 0.0                 0.0                40  \n",
       "4            0.0                 0.0                 0.0                40  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R-squared for LightGBM is 0.738391\n"
     ]
    }
   ],
   "source": [
    "lgb_params = {\n",
    "               'feature_fraction': 0.75,\n",
    "               'metric': 'rmse',\n",
    "               'nthread':1, \n",
    "               'min_data_in_leaf': 2**7, \n",
    "               'bagging_fraction': 0.75, \n",
    "               'learning_rate': 0.03, \n",
    "               'objective': 'mse', \n",
    "               'bagging_seed': 2**7, \n",
    "               'num_leaves': 2**7,\n",
    "               'bagging_freq':1,\n",
    "               'verbose':0 \n",
    "              }\n",
    "\n",
    "model = lgb.train(lgb_params, lgb.Dataset(X_train, label=y_train), 100)\n",
    "pred_lgb = model.predict(X_test)\n",
    "\n",
    "print('Test R-squared for LightGBM is %f' % r2_score(y_test, pred_lgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, concatenate test predictions to get test meta-features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 13.45896153,  13.37831474],\n",
       "       [  3.18599444,   2.55590212],\n",
       "       [  2.5028209 ,   1.52356814],\n",
       "       ..., \n",
       "       [  0.69860529,   0.41663964],\n",
       "       [  0.12072911,   0.34056468],\n",
       "       [  0.1755516 ,   0.32987826]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_level2 = np.c_[pred_lr, pred_lgb] \n",
    "# np.c_: This is short-hand for ``np.r_['-1,2,0', index expression]``, which is\n",
    "# useful because of its common occurrence. In particular, arrays will be\n",
    "# stacked along their last axis after being upgraded to at least 2-D with\n",
    "# 1's post-pended to the shape (column vectors made out of 1-D arrays).\n",
    "X_test_level2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train meta-features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now it is your turn to write the code**. You need to implement ***scheme f)*** from the reading material. Here, we will use duration **T** equal to month and **M=15**.  \n",
    "\n",
    "That is, you need to get predictions (meta-features) from *linear regression* and *LightGBM* for months 27, 28, 29, 30, 31, 32. Use the same parameters as in above models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_train_level2 = dates_train[dates_train.isin([27, 28, 29, 30, 31, 32])]\n",
    "\n",
    "# That is how we get target for the 2nd level dataset\n",
    "y_train_level2 = y_train[dates_train.isin([27, 28, 29, 30, 31, 32])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120192    27\n",
       "120193    27\n",
       "120194    27\n",
       "120195    27\n",
       "120196    27\n",
       "120197    27\n",
       "120198    27\n",
       "120199    27\n",
       "120200    27\n",
       "120201    27\n",
       "120202    27\n",
       "120203    27\n",
       "120204    27\n",
       "120205    27\n",
       "120206    27\n",
       "120207    27\n",
       "120208    27\n",
       "120209    27\n",
       "120210    27\n",
       "120211    27\n",
       "120212    27\n",
       "120213    27\n",
       "120214    27\n",
       "120215    27\n",
       "120216    27\n",
       "120217    27\n",
       "120218    27\n",
       "120219    27\n",
       "120220    27\n",
       "120221    27\n",
       "120222    27\n",
       "120223    27\n",
       "120224    27\n",
       "120225    27\n",
       "120226    27\n",
       "120227    27\n",
       "120228    27\n",
       "120229    27\n",
       "120230    27\n",
       "120231    27\n",
       "120232    27\n",
       "120233    27\n",
       "120234    27\n",
       "120235    27\n",
       "120236    27\n",
       "120237    27\n",
       "120238    27\n",
       "120239    27\n",
       "120240    27\n",
       "120241    27\n",
       "120242    27\n",
       "120243    27\n",
       "120244    27\n",
       "120245    27\n",
       "120246    27\n",
       "120247    27\n",
       "120248    27\n",
       "120249    27\n",
       "120250    27\n",
       "120251    27\n",
       "120252    27\n",
       "120253    27\n",
       "120254    27\n",
       "120255    27\n",
       "120256    27\n",
       "120257    27\n",
       "120258    27\n",
       "120259    27\n",
       "120260    27\n",
       "120261    27\n",
       "120262    27\n",
       "120263    27\n",
       "120264    27\n",
       "120265    27\n",
       "120266    27\n",
       "120267    27\n",
       "120268    27\n",
       "120269    27\n",
       "120270    27\n",
       "120271    27\n",
       "120272    27\n",
       "120273    27\n",
       "120274    27\n",
       "120275    27\n",
       "120276    27\n",
       "120277    27\n",
       "120278    27\n",
       "120279    27\n",
       "120280    27\n",
       "120281    27\n",
       "120282    27\n",
       "120283    27\n",
       "120284    27\n",
       "120285    27\n",
       "120286    27\n",
       "120287    27\n",
       "120288    27\n",
       "120289    27\n",
       "120290    27\n",
       "120291    27\n",
       "120292    27\n",
       "120293    27\n",
       "120294    27\n",
       "120295    27\n",
       "120296    27\n",
       "120297    27\n",
       "120298    27\n",
       "120299    27\n",
       "120300    27\n",
       "120301    27\n",
       "120302    27\n",
       "120303    27\n",
       "120304    27\n",
       "120305    27\n",
       "120306    27\n",
       "120307    27\n",
       "120308    27\n",
       "120309    27\n",
       "120310    27\n",
       "120311    27\n",
       "120312    27\n",
       "120313    27\n",
       "120314    27\n",
       "120315    27\n",
       "120316    27\n",
       "120317    27\n",
       "120318    27\n",
       "120319    27\n",
       "120320    27\n",
       "120321    27\n",
       "120322    27\n",
       "120323    27\n",
       "120324    27\n",
       "120325    27\n",
       "120326    27\n",
       "120327    27\n",
       "120328    27\n",
       "120329    27\n",
       "120330    27\n",
       "120331    27\n",
       "120332    27\n",
       "120333    27\n",
       "120334    27\n",
       "120335    27\n",
       "120336    27\n",
       "120337    27\n",
       "120338    27\n",
       "120339    27\n",
       "120340    27\n",
       "120341    27\n",
       "120342    27\n",
       "120343    27\n",
       "120344    27\n",
       "120345    27\n",
       "120346    27\n",
       "120347    27\n",
       "120348    27\n",
       "120349    27\n",
       "120350    27\n",
       "120351    27\n",
       "120352    27\n",
       "120353    27\n",
       "120354    27\n",
       "120355    27\n",
       "120356    27\n",
       "120357    27\n",
       "120358    27\n",
       "120359    27\n",
       "120360    27\n",
       "120361    27\n",
       "120362    27\n",
       "120363    27\n",
       "120364    27\n",
       "120365    27\n",
       "120366    27\n",
       "120367    27\n",
       "120368    27\n",
       "120369    27\n",
       "120370    27\n",
       "120371    27\n",
       "120372    27\n",
       "120373    27\n",
       "120374    27\n",
       "120375    27\n",
       "120376    27\n",
       "120377    27\n",
       "120378    27\n",
       "120379    27\n",
       "120380    27\n",
       "120381    27\n",
       "120382    27\n",
       "120383    27\n",
       "120384    27\n",
       "120385    27\n",
       "120386    27\n",
       "120387    27\n",
       "120388    27\n",
       "120389    27\n",
       "120390    27\n",
       "120391    27\n",
       "120392    27\n",
       "120393    27\n",
       "120394    27\n",
       "120395    27\n",
       "120396    27\n",
       "120397    27\n",
       "120398    27\n",
       "120399    27\n",
       "120400    27\n",
       "120401    27\n",
       "120402    27\n",
       "120403    27\n",
       "120404    27\n",
       "120405    27\n",
       "120406    27\n",
       "120407    27\n",
       "120408    27\n",
       "120409    27\n",
       "120410    27\n",
       "120411    27\n",
       "120412    27\n",
       "120413    27\n",
       "120414    27\n",
       "120415    27\n",
       "120416    27\n",
       "120417    27\n",
       "120418    27\n",
       "120419    27\n",
       "120420    27\n",
       "120421    27\n",
       "120422    27\n",
       "120423    27\n",
       "120424    27\n",
       "120425    27\n",
       "120426    27\n",
       "120427    27\n",
       "120428    27\n",
       "120429    27\n",
       "120430    27\n",
       "120431    27\n",
       "120432    27\n",
       "120433    27\n",
       "120434    27\n",
       "120435    27\n",
       "120436    27\n",
       "120437    27\n",
       "120438    27\n",
       "120439    27\n",
       "120440    27\n",
       "120441    27\n",
       "120442    27\n",
       "120443    27\n",
       "120444    27\n",
       "120445    27\n",
       "120446    27\n",
       "120447    27\n",
       "120448    27\n",
       "120449    27\n",
       "120450    27\n",
       "120451    27\n",
       "120452    27\n",
       "120453    27\n",
       "120454    27\n",
       "120455    27\n",
       "120456    27\n",
       "120457    27\n",
       "120458    27\n",
       "120459    27\n",
       "120460    27\n",
       "120461    27\n",
       "120462    27\n",
       "120463    27\n",
       "120464    27\n",
       "120465    27\n",
       "120466    27\n",
       "120467    27\n",
       "120468    27\n",
       "120469    27\n",
       "120470    27\n",
       "120471    27\n",
       "120472    27\n",
       "120473    27\n",
       "120474    27\n",
       "120475    27\n",
       "120476    27\n",
       "120477    27\n",
       "120478    27\n",
       "120479    27\n",
       "120480    27\n",
       "120481    27\n",
       "120482    27\n",
       "120483    27\n",
       "120484    27\n",
       "120485    27\n",
       "120486    27\n",
       "120487    27\n",
       "120488    27\n",
       "120489    27\n",
       "120490    27\n",
       "120491    27\n",
       "          ..\n",
       "154296    32\n",
       "154297    32\n",
       "154298    32\n",
       "154299    32\n",
       "154300    32\n",
       "154301    32\n",
       "154302    32\n",
       "154303    32\n",
       "154304    32\n",
       "154305    32\n",
       "154306    32\n",
       "154307    32\n",
       "154308    32\n",
       "154309    32\n",
       "154310    32\n",
       "154311    32\n",
       "154312    32\n",
       "154313    32\n",
       "154314    32\n",
       "154315    32\n",
       "154316    32\n",
       "154317    32\n",
       "154318    32\n",
       "154319    32\n",
       "154320    32\n",
       "154321    32\n",
       "154322    32\n",
       "154323    32\n",
       "154324    32\n",
       "154325    32\n",
       "154326    32\n",
       "154327    32\n",
       "154328    32\n",
       "154329    32\n",
       "154330    32\n",
       "154331    32\n",
       "154332    32\n",
       "154333    32\n",
       "154334    32\n",
       "154335    32\n",
       "154336    32\n",
       "154337    32\n",
       "154338    32\n",
       "154339    32\n",
       "154340    32\n",
       "154341    32\n",
       "154342    32\n",
       "154343    32\n",
       "154344    32\n",
       "154345    32\n",
       "154346    32\n",
       "154347    32\n",
       "154348    32\n",
       "154349    32\n",
       "154350    32\n",
       "154351    32\n",
       "154352    32\n",
       "154353    32\n",
       "154354    32\n",
       "154355    32\n",
       "154356    32\n",
       "154357    32\n",
       "154358    32\n",
       "154359    32\n",
       "154360    32\n",
       "154361    32\n",
       "154362    32\n",
       "154363    32\n",
       "154364    32\n",
       "154365    32\n",
       "154366    32\n",
       "154367    32\n",
       "154368    32\n",
       "154369    32\n",
       "154370    32\n",
       "154371    32\n",
       "154372    32\n",
       "154373    32\n",
       "154374    32\n",
       "154375    32\n",
       "154376    32\n",
       "154377    32\n",
       "154378    32\n",
       "154379    32\n",
       "154380    32\n",
       "154381    32\n",
       "154382    32\n",
       "154383    32\n",
       "154384    32\n",
       "154385    32\n",
       "154386    32\n",
       "154387    32\n",
       "154388    32\n",
       "154389    32\n",
       "154390    32\n",
       "154391    32\n",
       "154392    32\n",
       "154393    32\n",
       "154394    32\n",
       "154395    32\n",
       "154396    32\n",
       "154397    32\n",
       "154398    32\n",
       "154399    32\n",
       "154400    32\n",
       "154401    32\n",
       "154402    32\n",
       "154403    32\n",
       "154404    32\n",
       "154405    32\n",
       "154406    32\n",
       "154407    32\n",
       "154408    32\n",
       "154409    32\n",
       "154410    32\n",
       "154411    32\n",
       "154412    32\n",
       "154413    32\n",
       "154414    32\n",
       "154415    32\n",
       "154416    32\n",
       "154417    32\n",
       "154418    32\n",
       "154419    32\n",
       "154420    32\n",
       "154421    32\n",
       "154422    32\n",
       "154423    32\n",
       "154424    32\n",
       "154425    32\n",
       "154426    32\n",
       "154427    32\n",
       "154428    32\n",
       "154429    32\n",
       "154430    32\n",
       "154431    32\n",
       "154432    32\n",
       "154433    32\n",
       "154434    32\n",
       "154435    32\n",
       "154436    32\n",
       "154437    32\n",
       "154438    32\n",
       "154439    32\n",
       "154440    32\n",
       "154441    32\n",
       "154442    32\n",
       "154443    32\n",
       "154444    32\n",
       "154445    32\n",
       "154446    32\n",
       "154447    32\n",
       "154448    32\n",
       "154449    32\n",
       "154450    32\n",
       "154451    32\n",
       "154452    32\n",
       "154453    32\n",
       "154454    32\n",
       "154455    32\n",
       "154456    32\n",
       "154457    32\n",
       "154458    32\n",
       "154459    32\n",
       "154460    32\n",
       "154461    32\n",
       "154462    32\n",
       "154463    32\n",
       "154464    32\n",
       "154465    32\n",
       "154466    32\n",
       "154467    32\n",
       "154468    32\n",
       "154469    32\n",
       "154470    32\n",
       "154471    32\n",
       "154472    32\n",
       "154473    32\n",
       "154474    32\n",
       "154475    32\n",
       "154476    32\n",
       "154477    32\n",
       "154478    32\n",
       "154479    32\n",
       "154480    32\n",
       "154481    32\n",
       "154482    32\n",
       "154483    32\n",
       "154484    32\n",
       "154485    32\n",
       "154486    32\n",
       "154487    32\n",
       "154488    32\n",
       "154489    32\n",
       "154490    32\n",
       "154491    32\n",
       "154492    32\n",
       "154493    32\n",
       "154494    32\n",
       "154495    32\n",
       "154496    32\n",
       "154497    32\n",
       "154498    32\n",
       "154499    32\n",
       "154500    32\n",
       "154501    32\n",
       "154502    32\n",
       "154503    32\n",
       "154504    32\n",
       "154505    32\n",
       "154506    32\n",
       "154507    32\n",
       "154508    32\n",
       "154509    32\n",
       "154510    32\n",
       "154511    32\n",
       "154512    32\n",
       "154513    32\n",
       "154514    32\n",
       "154515    32\n",
       "154516    32\n",
       "154517    32\n",
       "154518    32\n",
       "154519    32\n",
       "154520    32\n",
       "154521    32\n",
       "154522    32\n",
       "154523    32\n",
       "154524    32\n",
       "154525    32\n",
       "154526    32\n",
       "154527    32\n",
       "154528    32\n",
       "154529    32\n",
       "154530    32\n",
       "154531    32\n",
       "154532    32\n",
       "154533    32\n",
       "154534    32\n",
       "154535    32\n",
       "154536    32\n",
       "154537    32\n",
       "154538    32\n",
       "154539    32\n",
       "154540    32\n",
       "154541    32\n",
       "154542    32\n",
       "154543    32\n",
       "154544    32\n",
       "154545    32\n",
       "154546    32\n",
       "154547    32\n",
       "154548    32\n",
       "154549    32\n",
       "154550    32\n",
       "154551    32\n",
       "154552    32\n",
       "154553    32\n",
       "154554    32\n",
       "154555    32\n",
       "154556    32\n",
       "154557    32\n",
       "154558    32\n",
       "154559    32\n",
       "154560    32\n",
       "154561    32\n",
       "154562    32\n",
       "154563    32\n",
       "154564    32\n",
       "154565    32\n",
       "154566    32\n",
       "154567    32\n",
       "154568    32\n",
       "154569    32\n",
       "154570    32\n",
       "154571    32\n",
       "154572    32\n",
       "154573    32\n",
       "154574    32\n",
       "154575    32\n",
       "154576    32\n",
       "154577    32\n",
       "154578    32\n",
       "154579    32\n",
       "154580    32\n",
       "154581    32\n",
       "154582    32\n",
       "154583    32\n",
       "154584    32\n",
       "154585    32\n",
       "154586    32\n",
       "154587    32\n",
       "154588    32\n",
       "154589    32\n",
       "154590    32\n",
       "154591    32\n",
       "154592    32\n",
       "154593    32\n",
       "154594    32\n",
       "154595    32\n",
       "Name: date_block_num, Length: 34404, dtype: int32"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates_train_level2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 11.,   0.,   3., ...,   0.,   0.,   0.], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_level2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "# And here we create 2nd level feeature matrix, init it with zeros first\n",
    "X_train_level2 = np.zeros([y_train_level2.shape[0], 2])\n",
    "\n",
    "# Now fill `X_train_level2` with metafeatures\n",
    "for cur_block_num in [27, 28, 29, 30, 31, 32]:\n",
    "    \n",
    "    print(cur_block_num)\n",
    "    \n",
    "    '''\n",
    "        1. Split `X_train` into parts\n",
    "           Remember, that corresponding dates are stored in `dates_train` \n",
    "        2. Fit linear regression \n",
    "        3. Fit LightGBM and put predictions          \n",
    "        4. Store predictions from 2. and 3. in the right place of `X_train_level2`. \n",
    "           You can use `dates_train_level2` for it\n",
    "           Make sure the order of the meta-features is the same as in `X_test_level2`\n",
    "    '''      \n",
    "    \n",
    "    #  YOUR CODE GOES HERE\n",
    "    # 1. Split `X_train` into parts\n",
    "#     X_train_part = X_train.loc[(dates <  cur_block_num) & (dates >=  cur_block_num-15)]\n",
    "#     y_train_part = all_data.loc[(dates <  cur_block_num) & (dates >=  cur_block_num-15), 'target'].values\n",
    "    X_train_part = X_train.loc[(dates <  cur_block_num)]\n",
    "    y_train_part = all_data.loc[(dates <  cur_block_num), 'target'].values\n",
    "    X_test_part = X_train.loc[(dates ==  cur_block_num)]\n",
    "    \n",
    "    # 2. Fit linear regression\n",
    "    lr.fit(X_train_part.values, y_train_part)\n",
    "    pred_lr = lr.predict(X_test_part.values)\n",
    "    \n",
    "    # 3. Fit LightGBM and put predictions\n",
    "    model = lgb.train(lgb_params, lgb.Dataset(X_train_part, label=y_train_part), 100)\n",
    "    pred_lgb = model.predict(X_test_part)\n",
    "    \n",
    "    # 4. Store predictions from 2. and 3. in the right place of `X_train_level2`. \n",
    "    # You can use `dates_train_level2` for it\n",
    "    # Make sure the order of the meta-features is the same as in `X_test_level2`\n",
    "    X_train_level2[dates_train_level2==cur_block_num] = np.c_[pred_lr, pred_lgb]\n",
    "    \n",
    "# Sanity check\n",
    "assert np.all(np.isclose(X_train_level2.mean(axis=0), [ 1.50148988,  1.38811989]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.19155666,  1.31854456],\n",
       "       [ 1.03517197,  0.99128266],\n",
       "       [ 1.55685619,  1.13375092],\n",
       "       ..., \n",
       "       [ 1.51231015,  1.34337936],\n",
       "       [ 1.77000441,  0.56668534],\n",
       "       [ 6.1339921 ,  0.51973077]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_level2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.50148988,  1.38811989])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_level2.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, the ensembles work best, when first level models are diverse. We can qualitatively analyze the diversity by examinig *scatter plot* between the two metafeatures. Plot the scatter plot below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGeFJREFUeJzt3X+Q3PV93/Hni+OAKzg5MGcGDmQJRlYGW6mEt0BHMbVx\ngjB0wpl6DBondRJPFbcmEzeuainu2DjBg2JCnGSa2sU1NTQOP2zwmdgkMrWYMGUCzsknJGFQLH4F\nFhldjM8/b7A4vfvHfvdYHbt3+/v7Y1+PmZvb/ex3d9/6Snrvd9+fX4oIzMysuI5JOwAzM+stJ3oz\ns4JzojczKzgnejOzgnOiNzMrOCd6M7OCc6I3Mys4J3ozs4JzojczK7hj0w4A4NRTT42VK1emHYaZ\nWa7s2rXrnyNibLnjMpHoV65cydTUVNphmJnliqRnmjnOpRszs4JbNtFLulnSIUn7atrukLQ7+Xla\n0u6kfaWkuZrHPtPL4M3MbHnNlG4+D/x34NZqQ0RcVb0t6UbgBzXHPxER67oVoJmZdWbZRB8RD0ha\nWe8xSQLeDVzc3bDMzKxbOq3RvwV4ISK+U9O2StK0pL+T9JZGT5S0WdKUpKmZmZkOwzAzs0Y6HXWz\nCbit5v5BYEVEfE/Sm4FJSW+MiB8ufmJE3ATcBFAqlbz7iVlOTU6XuWHHfp6fneOM0RG2bFzDxPrx\ntMOyGm0neknHAlcCb662RcRLwEvJ7V2SngDeAHjspFkBTU6X2Xb3XuYOzwNQnp1j2917AZzsM6ST\n0s0vA49HxHPVBkljkoaS22cDq4EnOwvRzLLqhh37F5J81dzheW7YsT+liKyeZoZX3gb8PbBG0nOS\n3pc8dDVHl20ALgL2JMMtvwS8PyJe7GbAZpYdz8/OtdRu6Whm1M2mBu2/UaftLuCuzsMyszw4Y3SE\ncp2kfsboSArRWCOeGWtmbduycQ0jw0NHtY0MD7Fl45qUIrJ6MrHWjZnlU7XD1aNuss2J3sw6MrF+\n3Ik941y6MTMrOCd6M7OCc6I3Mys4J3ozs4JzojczKzgnejOzgnOiNzMrOCd6M7OCc6I3Mys4J3oz\ns4JzojczKzgnejOzgnOiNzMrOCd6M7OCc6I3Myu4ZvaMvVnSIUn7atqulVSWtDv5uazmsW2SDkja\nL2ljrwI3M7PmNHNF/3ng0jrtn4qIdcnPvQCSzqWyafgbk+f8D0lDdZ5rZmZ90szm4A9IWtnk610B\n3B4RLwFPSToAnA/8fdsRmlnmTU6XvZ1ghnVSo79G0p6ktHNy0jYOPFtzzHNJm5kV1OR0mW1376U8\nO0cA5dk5tt29l8npctqhWaLdRP9p4BxgHXAQuLHVF5C0WdKUpKmZmZk2wzCztN2wYz9zh+ePaps7\nPM8NO/anFJEt1laij4gXImI+Io4An6VSngEoA2fVHHpm0lbvNW6KiFJElMbGxtoJw8wy4PnZuZba\nrf/aSvSSTq+5+06gOiLnHuBqScdLWgWsBr7ZWYhmlmVnjI601G7918zwytuodKaukfScpPcBn5S0\nV9Ie4G3AfwaIiEeBO4FvA38LfCAi5hu8tJkVwJaNaxgZPnpw3cjwEFs2rkkpIltMEZF2DJRKpZia\nmko7DDNrk0fdpEPSrogoLXfcssMrzcyWM7F+3Ik9w5zozbrAV7SWZU70Zh2qjiOvDjGsjiMHnOwt\nE7yomVmHPI7css5X9GYdKvI48ryVpPIWb7840Zt16IzREcp1knrex5HnrSSVt3j7yaUbsw4VdRx5\n3kpSeYu3n3xFb9ah6tVi0UoGeStJ5S3efnKiN+uCvI0jb6aWnbeSVN7i7SeXbswKanK6zIbtO1m1\n9Wts2L5zYdngZpcVzltJKm/x9pOv6M0KaKmOyaVq2bVX9XkrSeUt3n7yWjdmBbRh+866ZYzx0RGe\nT67kFxPw1PbLex6bdU+za924dGNWQEt1THpZ4cHjRG9WQEslc9eys6FRH0ovONFnQD//wm0wLJXM\nJ9aPc/2VaxkfHUFUyjnXX7nWtew+6vc+u+6MTZln81kvLNcxmbfhoEXTbId4tzjRp6zff+E2OJzM\ns6vfk7tcukmZZ/OZDZ5+d4g70afMIyDMBk+/O8Sb2Rz8ZkmHJO2rabtB0uOS9kj6sqTRpH2lpDlJ\nu5Ofz/Qk6gLxCAiz/srC4Id+d4gvO2FK0kXAj4FbI+JNSdslwM6IeFnSHwFExIclrQS+Wj2uWYM+\nYcpraJv1x+LBD1C5sMrrqKOubQ4eEQ8kCby27es1dx8C3tVqgPYKd5qZ9cegDn7oRo3+t4C/qbm/\nStK0pL+T9JZGT5K0WdKUpKmZmZkuhGFmtrRBHfzQUaKX9BHgZeALSdNBYEVErAd+D/grST9X77kR\ncVNElCKiNDY21kkYZmZNGdTBD20nekm/Afxb4D2RFPoj4qWI+F5yexfwBPCGLsRpZtaxQR380NaE\nKUmXAv8V+DcR8dOa9jHgxYiYl3Q2sBp4siuRmpl1aFCXMl420Uu6DXgrcKqk54CPAduA44H7JAE8\nFBHvBy4C/kDSYeAI8P6IeLFHsZuZtWwQBz80M+pmU53mzzU49i7grk6DMjOz7vHMWDOzgnOiNzMr\nOK9eaR3xrF6z7HOit7Z5LX2zfHDpxtq21HRyM8sOJ3pr26BOJzfLGyd6a9ugTic3yxsnemvboE4n\nN8sbd8Za2wZ1OrlZ3jjRW0cGcTq5Wd64dGNmVnBO9GZmBefSTR949qiZpcmJvsc8e9TM0ubSTY95\n9qiZpc2Jvsc8e9TM0uZE32OePWpmaXOi7zHPHrVGJqfLbNi+k1Vbv8aG7TuZnC6nHZIVVFOJXtLN\nkg5J2lfTdoqk+yR9J/l9ctIuSX8u6YCkPZLO61XweTCxfpzrr1zL+OgIAsZHR7j+yrXuiB1w1U76\n8uwcwSud9E721guKiOUPki4CfgzcGhFvSto+CbwYEdslbQVOjogPS7oM+B3gMuAC4M8i4oKlXr9U\nKsXU1FSHfxSz/NiwfSflOv0046MjPLj14hQisjyStCsiSssd19QVfUQ8ALy4qPkK4Jbk9i3ARE37\nrVHxEDAq6fTmwjYbDO6kt37qpEZ/WkQcTG5/FzgtuT0OPFtz3HNJm5kl3Elv/dSVztio1H+WrwHV\nkLRZ0pSkqZmZmW6EYZYb7qS3fuok0b9QLckkvw8l7WXgrJrjzkzajhIRN0VEKSJKY2NjHYRhlj/u\npLd+6mQJhHuA9wLbk99fqWm/RtLtVDpjf1BT4jGzhJd4tn5pKtFLug14K3CqpOeAj1FJ8HdKeh/w\nDPDu5PB7qYy4OQD8FPjNLsdsZmYtaCrRR8SmBg+9vc6xAXygk6DMzKx7PDPWzKzgnOjNzArOid7M\nrOCc6M3MCs6J3sys4JzozcwKznvG9pE3CTezNDjRt6jdZO1Nws0sLU70LWgnWVc/GOqtPV7dJNyJ\n3sx6yTX6FtywY/9Ckq+qJut6ancRasTrj5tZrznRt6DVzSLqfTAs5vXHzazXnOhb0OpmEctdrXv9\ncTPrByf6FrS6WcRSV+tef9zM+sWdsS2oJuVmR91s2bjmqM5bqHwwZDnBewioWfE40beolc0iWv1g\nSJuHgJoVkxN9j+VpF6GlRhXl5c9gZq/mGr0taHVUkZnlgxO9LWh1VJGZ5YMTvS1odVSRmeVD2zV6\nSWuAO2qazgY+CowC/wGYSdp/PyLubTvCAsjLSJa8dR6bWXNU2cu7wxeRhoAycAHwm8CPI+KPm31+\nqVSKqampjuPIosUjWSD7QyzNLB8k7YqI0nLHdat083bgiYh4pkuvVxitro9jZtZt3Ur0VwO31dy/\nRtIeSTdLOrlL75FLHsliZmnrONFLOg74VeCLSdOngXOAdcBB4MYGz9ssaUrS1MzMTL1DCsEjWcws\nbd24on8H8K2IeAEgIl6IiPmIOAJ8Fji/3pMi4qaIKEVEaWxsrAthZJNHsphZ2roxM3YTNWUbSadH\nxMHk7juBfV14j9zySBYzS1tHiV7SicCvAL9d0/xJSeuAAJ5e9NhAytMyCGZWPB0l+oj4CfDaRW2/\n3lFEZmbWVZ4Za2ZWcE70ZmYF50RvZlZwTvRmZgXnRG9mVnBO9GZmBedEb2ZWcE70ZmYF583BeyQv\nm42YWfE50ffA4s1GyrNzbLt7L4CTvZn1nRP9Mtq5Ml9qsxEnejPrNyf6JbR7Ze7NRswsS5zol7Dc\nNoCNrvTPGB2hXCepe7MRM0uDR93UMTldZsP2nXWTNbxyZV+enSNq7k9OlwFvNmJm2eJEv0i1XNMo\nyQMMSUte6U+sH+f6K9cyPjqCgPHREa6/cq3r82aWCpduFqlXrqk1MjzU8PHaGrw3GzGzrPAV/SJL\nXckL+HdvHmfcG36bWY440deYnC6jJR4P4P7HZ1yDN7Nccemmxg079hPLHPP87Jw3/DazXOk40Ut6\nGvgRMA+8HBElSacAdwArqWwQ/u6I+H6n79VrzYxzr5ZnXIM3s7zoVunmbRGxLiJKyf2twDciYjXw\njeR+5i1XY3d5xszyqFc1+iuAW5LbtwATPXqfrqpXe6/W7D1E0szyqhs1+gC+LimA/xkRNwGnRcTB\n5PHvAqctfpKkzcBmgBUrVnQhjM659m5mRaSI5bofl3kBaTwiypJeB9wH/A5wT0SM1hzz/Yg4udFr\nlEqlmJqa6igOM7NBI2lXTcm8oY6v6COinPw+JOnLwPnAC5JOj4iDkk4HDnX6PlnT7nrzjZ7n9evN\nrFc6SvSSTgSOiYgfJbcvAf4AuAd4L7A9+f2VTgNNw+R0mY//9aN8/6eHARgdGebaX30jQFurWjZa\nDXPqmRe5a1fZ69ebWU90VLqRdDbw5eTuscBfRcQnJL0WuBNYATxDZXjli41eJ4ulm8npMlu+9AiH\n548+P8PHiJNOOHYh+dczJLHpgrO4bmLtUe2NFkobkpiv8/cwPjrCg1svbvNPcDR/YzArnr6UbiLi\nSeBf1mn/HvD2Tl47bTfs2P+qJA9w+EgsmeQB5iP4y4f+CYDrJtYuJNlGyyvUS/LQvfXrveOV2WDz\nzNgGupFkb3v4WUqvP+WoJFtPoyv6bq2d0+66+mZWDF7rpoHlkuxSa+JUzUc0tRrmpgvO6unaOY0+\ntJZbV79WdY3+VVu/xobtO+seY2bZ5ETfwJaNaxgeapzOm+nZGJKW/GZQnYR13cTanq5f3+hDa7l1\n9atq1+hf7gPBzLLHpZsGqkm2dtRNqzZdcBb3Pz5Ttza/uKO1l2vnbNm45lXlo2bX1Qdvdm6Wd76i\nX8LE+nGmP3oJT2+/vKlSTa0N55zCdRNrM7GkcaMdr5pdV9+bnZvlm6/om9Row29xdBlHwHsuXLEw\ntDIryyo0+sZQ70p/8YeQNzs3yzcn+iZt2biGLV98hMNHXknrw8eIq86vlGeWSuJZXdK42Q+hRqUf\nr+Rplg9O9ImmJhQtrt8ISq8/5VUTo/KkmQ+hrHwrMbP2ONHT3ISiehOoDs/HwHRIZvVbiZktb+A7\nYyeny3zozkeWHWboDkkzy6uBvqKvXsk3WoKgPDvHOdvuZT6i57NXzcx6ZaCv6JebtQqvrENTL8m7\nQ9LM8mCgr+gbLTLWjHF3SJpZTgz0Ff2QWp0G9YqfvPRyFyMxM+udgU70jWrzzZidO+z1XswsFwYy\n0VdXYuzU3OF5PnTnI17R0cwybeBq9I12jmpX9VuBN/Mws6wauCv6j//1o11L8ovVW+LXzCxtbSd6\nSWdJul/StyU9Kul3k/ZrJZUl7U5+LuteuJ2ZnC63vORwqx22nkBlZlnTSenmZeBDEfEtSa8Bdkm6\nL3nsUxHxx52H1z3VyVGtarXD1hOozCxr2k70EXEQOJjc/pGkx4DMFqebmRzVKU+gMrMs6kqNXtJK\nYD3wcNJ0jaQ9km6WdHI33qMTk9PljiZHLeUY0ZPt/8zMuqXjUTeSTgLuAj4YET+U9GngD6nsx/GH\nwI3Ab9V53mZgM8CKFSvaeu/q0sLl2bmFtWjGR0d42y+MLawR//Mjw/zkZ72b3PRzJwyz+2OX9Oz1\nzcw6pehg0pCkYeCrwI6I+JM6j68EvhoRb1rqdUqlUkxNTbX03ouXFk6LgKe2X55qDGY2mCTtiojS\ncsd1MupGwOeAx2qTvKTTaw57J7Cv3fdYSj9q7s1w56uZZV0npZsNwK8DeyXtTtp+H9gkaR2V0s3T\nwG93FGEDWRjGOHyM3PlqZpnXyaib/8erN9cDuLf9cJrXaMPqfjrphGPd+WpmmZfbmbFbNq5hZHgo\n1RhmW5x8ZWaWhtwm+on141x/5VrGkxp5dQbr+OgIv3bhCsZHRxaGPf6L4d78MV2fN7M8yPWiZs1u\nWD05XeaDd+xe9rhWeHKUmeVFbq/oW9HtOronR5lZnuT6ir6R6kSq52fnOCPZ8m90ZJjZuc5r6gIe\n3Hpx50GamfVJ4a7oq2Wa8uwcQWWd+A/esZs3nvGarry+6/JmljeFS/SNavEPPvFix6/turyZ5VHh\nEn0vuS5vZnlUqETfyz1bh49pbQMSM7OsKFSiv/aeR3v22oePhLcJNLNcKlSi78aomqWkveSCmVk7\nCjm8sleqs2/rDd907d7MssqJvgXzEfy3yb3ctau8sERyeXZuYS9aJ3szy6JClW764QsP/dOr1sGf\nOzzv+r2ZZZYTfYsa7ceVhfXxzczqcaLvEs+YNbOscqJvw+IR9Z4xa2ZZ5kTfopHhId6zaL17z5g1\nsyzzqJsmDB8DLx/BQynNLJd6luglXQr8GTAE/K+I2N6r9wL4xY/9bc9e+0iIp7Zf1rPXNzPrpZ6U\nbiQNAX8BvAM4F9gk6dxevFfVD1+aX/6gNs1Ho7E2ZmbZ16sa/fnAgYh4MiJ+BtwOXNGj9+q56oxY\nM7M86lWiHweerbn/XNK2QNJmSVOSpmZmZnoURndsuuCstEMwM2tbaqNuIuKmiChFRGlsbCytMI4y\nJLH6dScuXMEPSfzahSu4bmJtypGZmbWvV52xZaD2MvjMpC2znt5+edohmJn1RK+u6P8BWC1plaTj\ngKuBe3r0XkBnido1eDMrsp4k+oh4GbgG2AE8BtwZEb3bFSTx9PbL+dOr1rX8PNfgzazIelajj4h7\nI+INEXFORHyiV++zWKuTmVa/7kTX4M2s0Aq5BMJ4kwuMrX7didz3e2/tbTBmZikrZKLfsnHNqxYe\nq+fAoZ/0dENxM7MsKGSin1g/znsuXLFssg/whiFmVniFTPQA102s5VNXrVu2jOMNQ8ys6Aqb6KFy\nZf/g1ov506vWNby694YhZlZ0hU70VY1KOd4wxMwGwUAkeji6lOMNQ8xskAzUxiMT68ed2M1s4AzM\nFb2Z2aByojczKzgnejOzgnOiNzMrOCd6M7OCU2Rg42tJM8AzXX7ZU4F/7vJrdpPj61zWY3R8ncl6\nfJB+jK+PiGW36MtEou8FSVMRUUo7jkYcX+eyHqPj60zW44N8xAgu3ZiZFZ4TvZlZwRU50d+UdgDL\ncHydy3qMjq8zWY8P8hFjcWv0ZmZWUeQrejMzo4CJXtKlkvZLOiBpa9rxVEl6WtJeSbslTSVtp0i6\nT9J3kt8n9zGemyUdkrSvpq1uPKr48+Sc7pF0XkrxXSupnJzD3ZIuq3lsWxLffkkb+xDfWZLul/Rt\nSY9K+t2kPRPncIn4snQOT5D0TUmPJDF+PGlfJenhJJY7JB2XtB+f3D+QPL4ypfg+L+mpmnO4Lmnv\n+/+TpkVEYX6AIeAJ4GzgOOAR4Ny040piexo4dVHbJ4Gtye2twB/1MZ6LgPOAfcvFA1wG/A0g4ELg\n4ZTiuxb4L3WOPTf5uz4eWJX8GxjqcXynA+clt18D/GMSRybO4RLxZekcCjgpuT0MPJycmzuBq5P2\nzwD/Mbn9n4DPJLevBu5IKb7PA++qc3zf/580+1O0K/rzgQMR8WRE/Ay4Hbgi5ZiWcgVwS3L7FmCi\nX28cEQ8ALzYZzxXArVHxEDAq6fQU4mvkCuD2iHgpIp4CDlD5t9AzEXEwIr6V3P4R8BgwTkbO4RLx\nNZLGOYyI+HFydzj5CeBi4EtJ++JzWD23XwLeLmm5raF7EV8jff9/0qyiJfpx4Nma+8+x9D/ufgrg\n65J2SdqctJ0WEQeT298FTksntAWN4snSeb0m+Vp8c02pK9X4khLCeipXfJk7h4vigwydQ0lDknYD\nh4D7qHyTmI2Il+vEsRBj8vgPgNf2M76IqJ7DTyTn8FOSjl8cX53YU1W0RJ9lvxQR5wHvAD4g6aLa\nB6Py3S8zQ6CyFk/i08A5wDrgIHBjuuGApJOAu4APRsQPax/LwjmsE1+mzmFEzEfEOuBMKt8gfiHN\neBZbHJ+kNwHbqMT5r4BTgA+nGGJTipboy8BZNffPTNpSFxHl5Pch4MtU/lG/UP1ql/w+lF6EsEQ8\nmTivEfFC8h/vCPBZXiktpBKfpGEqSfQLEXF30pyZc1gvvqydw6qImAXuB/41lZJHdfe72jgWYkwe\n/3nge32O79KkLBYR8RLwv8nIOVxK0RL9PwCrk17746h02NyTckxIOlHSa6q3gUuAfVRie29y2HuB\nr6QT4YJG8dwD/PtkVMGFwA9qyhN9s6je+U4q57Aa39XJqIxVwGrgmz2ORcDngMci4k9qHsrEOWwU\nX8bO4Zik0eT2CPArVPoS7gfelRy2+BxWz+27gJ3Jt6Z+xvd4zQe5qPQf1J7D1P+f1JV2b3C3f6j0\nfP8jlVrfR9KOJ4npbCojGh4BHq3GRaW++A3gO8D/BU7pY0y3UfnqfphKLfF9jeKhMorgL5Jzuhco\npRTf/0nefw+V/1Sn1xz/kSS+/cA7+hDfL1Epy+wBdic/l2XlHC4RX5bO4S8C00ks+4CPJu1nU/mQ\nOQB8ETg+aT8huX8gefzslOLbmZzDfcBf8srInL7/P2n2xzNjzcwKrmilGzMzW8SJ3sys4JzozcwK\nzonezKzgnOjNzArOid7MrOCc6M3MCs6J3sys4P4/D+TJ9SQCo5EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f640b3a6a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = X_train_level2[:,0]\n",
    "y = X_train_level2[:,1]\n",
    "plt.scatter(x, y)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when the meta-features are created, we can ensemble our first level models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple convex mix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with simple linear convex mix:\n",
    "\n",
    "$$\n",
    "mix= \\alpha\\cdot\\text{linreg_prediction}+(1-\\alpha)\\cdot\\text{lgb_prediction}\n",
    "$$\n",
    "\n",
    "We need to find an optimal $\\alpha$. And it is very easy, as it is feasible to do grid search. Next, find the optimal $\\alpha$ out of `alphas_to_try` array. Remember, that you need to use train meta-features (not test) when searching for $\\alpha$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.765000; Corresponding r2 score on train: 0.627255\n"
     ]
    }
   ],
   "source": [
    "alphas_to_try = np.linspace(0, 1, 1001)\n",
    "\n",
    "# YOUR CODE GOES HERE\n",
    "r2_score_list = []\n",
    "for alpha in alphas_to_try:\n",
    "    mix = alpha*X_train_level2[:,0] + (1-alpha)*(X_train_level2[:,1])\n",
    "    r2_score_list.append(r2_score(y_train_level2, mix))\n",
    "    \n",
    "best_alpha = alphas_to_try[r2_score_list.index(max(r2_score_list))]\n",
    "r2_train_simple_mix = max(r2_score_list)\n",
    "\n",
    "print('Best alpha: %f; Corresponding r2 score on train: %f' % (best_alpha, r2_train_simple_mix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the $\\alpha$ you've found to compute predictions for the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R-squared for simple mix is 0.781144\n"
     ]
    }
   ],
   "source": [
    "# From above:\n",
    "# X_test_level2 = np.c_[pred_lr, pred_lgb] \n",
    "\n",
    "mix = best_alpha*X_test_level2[:,0] + (1-best_alpha)*(X_test_level2[:,1])\n",
    "\n",
    "test_preds = mix # YOUR CODE GOES HERE\n",
    "r2_test_simple_mix = r2_score(y_test, mix) # YOUR CODE GOES HERE\n",
    "\n",
    "print('Test R-squared for simple mix is %f' % r2_test_simple_mix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will try a more advanced ensembling technique. Fit a linear regression model to the meta-features. Use the same parameters as in the model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# 2. Fit linear regression\n",
    "lr.fit(X_train_level2, y_train_level2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute R-squared on the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R-squared for stacking is 0.632176\n",
      "Test  R-squared for stacking is 0.771297\n"
     ]
    }
   ],
   "source": [
    "train_preds = lr.predict(X_train_level2) # YOUR CODE GOES HERE\n",
    "r2_train_stacking = r2_score(y_train_level2, train_preds) # YOUR CODE GOES HERE\n",
    "\n",
    "test_preds = lr.predict(X_test_level2) # YOUR CODE GOES HERE\n",
    "r2_test_stacking = r2_score(y_test, test_preds) # YOUR CODE GOES HERE\n",
    "\n",
    "print('Train R-squared for stacking is %f' % r2_train_stacking)\n",
    "print('Test  R-squared for stacking is %f' % r2_test_stacking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, that the score turned out to be lower than in previous method. Although the model is very simple (just 3 parameters) and, in fact, mixes predictions linearly, it looks like it managed to overfit. **Examine and compare** train and test scores for the two methods. \n",
    "\n",
    "And of course this particular case does not mean simple mix is always better than stacking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We all done! Submit everything we need to the grader now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task best_alpha is: 0.765\n",
      "Current answer for task r2_train_simple_mix is: 0.627255043446\n",
      "Current answer for task r2_test_simple_mix is: 0.781144169579\n",
      "Current answer for task r2_train_stacking is: 0.632175561459\n",
      "Current answer for task r2_test_stacking is: 0.771297132342\n"
     ]
    }
   ],
   "source": [
    "from grader import Grader\n",
    "grader = Grader()\n",
    "\n",
    "grader.submit_tag('best_alpha', best_alpha)\n",
    "\n",
    "grader.submit_tag('r2_train_simple_mix', r2_train_simple_mix)\n",
    "grader.submit_tag('r2_test_simple_mix',  r2_test_simple_mix)\n",
    "\n",
    "grader.submit_tag('r2_train_stacking', r2_train_stacking)\n",
    "grader.submit_tag('r2_test_stacking',  r2_test_stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You want to submit these numbers:\n",
      "Task best_alpha: 0.765\n",
      "Task r2_train_simple_mix: 0.627255043446\n",
      "Task r2_test_simple_mix: 0.781144169579\n",
      "Task r2_train_stacking: 0.632175561459\n",
      "Task r2_test_stacking: 0.771297132342\n"
     ]
    }
   ],
   "source": [
    "STUDENT_EMAIL = 'ngyibin@gmail.com' # EMAIL HERE\n",
    "STUDENT_TOKEN = 'qh0XC6YRZ524HsoK' # TOKEN HERE\n",
    "grader.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted to Coursera platform. See results on assignment page!\n"
     ]
    }
   ],
   "source": [
    "grader.submit(STUDENT_EMAIL, STUDENT_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "state": {
    "67c50563eff0405ab110b06e10a8e0cd": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
